# GenerativeAdversialNetwork
GAN is a battle between two adversaries, the generator and the discrminator.
The generator tries to convert random noise into observations so that it can fool the discriminator into thinking that it comes from the original dataset. The discriminator tries to predict whether an observation is real or fake (generated by the generator).

The key in GAN training is how we alternate the training of the two networks so that the generator makes more realistic observations and fools the discriminator, and the discriminator maintains its quality in distinguishing between real and fake observations.

## The Discriminator
The discriminator predicts if an observation is real or fake. The architecture is the same as a normal supervised binary classification, where the real observations have a label of one and the generated ones have a label of zero, with binary cross-entropy as the loss function.

![the discriminator takes samples from the original dataset and samples generated by the generator](https://github.com/hafifi29/GenerativeAdversialNetwork/assets/89405591/99399c9f-1060-45f7-9712-c8c1ad23829f)


## The Generator
The input to the generator is a vector drawn from a multivariate standard normal distribution, and the output is an observation of the same size as an observation in the original training data. _If you studied autoencoders, you can think of the generator as the decoder part of the autoencoder._

The discriminator generates a score for each image that passes through it. The loss function for the generator is then simply the binary cross-entropy between these probabilities and a vector of ones.

![the generator takes the loss from the discriminator to update its own weigths.](https://github.com/hafifi29/GenerativeAdversialNetwork/assets/89405591/b420f783-7c83-4c2f-9e59-2a221c4a21a3)

## The training process
The key to understanding GANs lies in understanding the training process. As we said before, the loss of the discriminator is the binary cross-entropy between the real and fake observations, with 1 for real and 0 for fake. and the loss for the generator is the binary cross-entropy between a vector of ones and the loss of the fake observation from the discriminator. Crucially, we must alternate the training process between the two networks and make sure to update the weights of the networks one at a time.

### Training Challenges
* Discriminator overpowers the generator
  
  If the discriminator becomes too powerful, the loss signal that goes to the generator becomes too weak to add much change to the generator weights. In the worst scenario, the discriminator becomes perfect at separating the observations, and the gradient vanishes completely, leading to no learning.
  
  how to weaken the discriminator
  1. increase the dropout rate
  2. reduce the learning ate of the discriminator
  3. add noise to the training labels of the discriminator
     
* Generator overpowers the discriminator
  
  If the discriminator is weak, the generator will find a way to trick the discriminator with a small sample of identical observations called the mode collapse. The generator will find a way to map every input to the mode without generating observations, and the gradient can collapse to near zero.To strengthen the discriminator, we use the opposite of the suggestions listed in the preceding part.

  
